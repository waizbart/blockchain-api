# Sistema de AnÃ¡lise de Severidade de DenÃºncias

## ğŸ“‹ VisÃ£o Geral

O sistema de anÃ¡lise de severidade classifica automaticamente as denÃºncias em quatro nÃ­veis:

- **CRITICA**: SituaÃ§Ãµes que envolvem risco iminente Ã  vida, crimes graves, emergÃªncias
- **ALTA**: Crimes significativos, violaÃ§Ãµes sÃ©rias de direitos, situaÃ§Ãµes de risco  
- **MEDIA**: InfraÃ§Ãµes moderadas, problemas administrativos sÃ©rios, irregularidades
- **BAIXA**: QuestÃµes administrativas menores, reclamaÃ§Ãµes de serviÃ§os, problemas rotineiros

## ğŸ¤– MÃ©todo de AnÃ¡lise

### AnÃ¡lise com LLM (Large Language Model)

- **Modelo PadrÃ£o**: GPT-4 da OpenAI
- **Mock para Desenvolvimento**: Adapter mock para testes
- **Vantagens**: CompreensÃ£o contextual avanÃ§ada, anÃ¡lise semÃ¢ntica, flexibilidade
- **Requisitos**: API key OpenAI para produÃ§Ã£o

## ğŸ—ï¸ Arquitetura

### PadrÃµes de Design Implementados

1. **Adapter Pattern**: Interface unificada para diferentes provedores LLM
2. **Factory Pattern**: CriaÃ§Ã£o centralizada de adapters LLM

### Estrutura de Arquivos

```
app/
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ llm_adapter.py          # Interface abstrata
â”‚   â””â”€â”€ openai_adapter.py       # ImplementaÃ§Ã£o OpenAI + Mock
â”œâ”€â”€ factories/
â”‚   â””â”€â”€ llm_factory.py          # Factory para LLMs
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ severity_analysis_prompts.py  # Prompts modulares
â”œâ”€â”€ services/
â”‚   â””â”€â”€ severity_analysis_service.py  # ServiÃ§o principal
â””â”€â”€ models/
    â””â”€â”€ denuncia.py             # Enum SeveridadeDenuncia
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# ConfiguraÃ§Ãµes LLM
OPENAI_API_KEY=sk-your-openai-key-here
LLM_PROVIDER=openai                    # ou 'mock' para desenvolvimento
LLM_MODEL=gpt-4                        # ou 'gpt-3.5-turbo'
```

### ConfiguraÃ§Ã£o por Ambiente

- **Desenvolvimento**: Usa mock LLM por padrÃ£o
- **ProduÃ§Ã£o**: Requer OpenAI configurado
- **Testes**: Sempre usa mock LLM

## ğŸš€ Uso da API

### Filtrar DenÃºncias por Severidade

```http
GET /api/denuncias?severidade=CRITICA
```

**ParÃ¢metros:**

- `severidade`: BAIXA, MEDIA, ALTA, CRITICA

### Resposta das DenÃºncias

```json
{
  "id": 123,
  "descricao": "DescriÃ§Ã£o da denÃºncia...",
  "categoria": "VIOLENCIA",
  "severidade": "ALTA",
  "status": "PENDING",
  "datetime": "2024-01-15T10:30:00Z",
  // ... outros campos
}
```

## ğŸ”§ AnÃ¡lise AutomÃ¡tica

### Quando Ocorre

- **CriaÃ§Ã£o de Nova DenÃºncia**: AnÃ¡lise automÃ¡tica na criaÃ§Ã£o
- **Processamento em Lote**: Para denÃºncias sem severidade definida
- **Re-anÃ¡lise Manual**: Via endpoints administrativos

### Fatores Considerados

1. **Gravidade dos Fatos**: Natureza e gravidade dos fatos relatados
2. **UrgÃªncia**: Necessidade de aÃ§Ã£o imediata
3. **Impacto Social**: Potencial impacto na sociedade
4. **Risco**: Riscos Ã  seguranÃ§a, saÃºde ou direitos
5. **Categoria**: Peso da categoria da denÃºncia
6. **Credibilidade**: HistÃ³rico do usuÃ¡rio
7. **Contexto Temporal e GeogrÃ¡fico**: AnÃ¡lise contextual da situaÃ§Ã£o

## ğŸ“Š Exemplos de ClassificaÃ§Ã£o

### CRITICA

- "AmeaÃ§a de bomba na escola municipal"
- "Tiroteio em andamento no centro da cidade"
- "Sequestro de crianÃ§a relatado"

### ALTA

- "AgressÃ£o fÃ­sica contra idoso"
- "CorrupÃ§Ã£o envolvendo milhÃµes em licitaÃ§Ã£o"
- "TrÃ¡fico de drogas prÃ³ximo Ã  escola"

### MEDIA

- "Furto de celular no transporte pÃºblico"
- "Fraude em benefÃ­cio social"
- "Vandalismo em prÃ©dio pÃºblico"

### BAIXA

- "MÃºsica alta do vizinho apÃ³s 22h"
- "Buraco na calÃ§ada sem sinalizaÃ§Ã£o"
- "Lixo acumulado em terreno baldio"

## ğŸ”’ Prompts e SeguranÃ§a

### ModularizaÃ§Ã£o de Prompts

- **Arquivo Central**: `app/prompts/severity_analysis_prompts.py`
- **Prompts EspecÃ­ficos**: Por categoria de denÃºncia
- **Versionamento**: Facilita ajustes e melhorias

### ValidaÃ§Ã£o de Respostas

- **SanitizaÃ§Ã£o**: ValidaÃ§Ã£o de campos obrigatÃ³rios
- **NormalizaÃ§Ã£o**: Mapeamento de valores similares
- **Fallback**: Valores padrÃ£o para respostas invÃ¡lidas

## ğŸ§ª Testes e Desenvolvimento

### Mock Adapter

```python
from app.factories.llm_factory import create_mock_adapter

# Usar para desenvolvimento/testes
llm = create_mock_adapter()
```

### Ambiente de Desenvolvimento

```python
from app.factories.llm_factory import EnvironmentLLMFactory

# Detecta automaticamente o melhor LLM disponÃ­vel
llm = EnvironmentLLMFactory.create_for_environment("development")
```

## ğŸ’° Custos e Performance

### Estimativa de Custos (OpenAI)

- **GPT-4**: ~$0.03-0.06 por anÃ¡lise
- **GPT-3.5-turbo**: ~$0.002-0.003 por anÃ¡lise
- **Mock**: $0.00

### Performance

- **LLM**: 2-5 segundos por anÃ¡lise
- **Mock**: <100ms por anÃ¡lise (desenvolvimento)

## ğŸ”„ Extensibilidade

### Adicionar Novo Provedor LLM

```python
from app.adapters.llm_adapter import LLMAdapter
from app.factories.llm_factory import LLMFactory

class NovoProviderAdapter(LLMAdapter):
    # Implementar mÃ©todos abstratos
    pass

# Registrar novo provedor
LLMFactory.register_provider("novo_provider", NovoProviderAdapter)
```

### Customizar Prompts

- Editar `app/prompts/severity_analysis_prompts.py`
- Adicionar prompts especÃ­ficos por categoria
- Implementar prompts multi-idioma

## ğŸ“ˆ Monitoramento

### MÃ©tricas Recomendadas

- Taxa de uso LLM vs Regras
- DistribuiÃ§Ã£o de severidades
- Tempo de resposta por mÃ©todo
- Custo por anÃ¡lise (LLM)
- Taxa de erro/fallback

### Logs

- DecisÃµes de classificaÃ§Ã£o
- Erros de LLM e fallbacks
- Performance por mÃ©todo
- Uso de recursos

## ğŸ› ï¸ ManutenÃ§Ã£o

### Ajuste de Prompts

- Monitorar qualidade das classificaÃ§Ãµes
- Ajustar prompts baseado em feedback
- Versionar mudanÃ§as importantes

### CalibraÃ§Ã£o de Regras

- Revisar palavras-chave periodicamente
- Ajustar pesos por categoria
- Sincronizar com classificaÃ§Ãµes LLM

### Backup e RedundÃ¢ncia

- Mock sempre disponÃ­vel como fallback
- Regras independentes de APIs externas
- Cache de resultados (recomendado)
